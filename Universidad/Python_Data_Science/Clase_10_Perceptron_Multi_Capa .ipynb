{"cells":[{"cell_type":"markdown","metadata":{"id":"9tMrNL2-dKph"},"source":["# Perceptrón Multicapa: Un análisis rápido"]},{"cell_type":"markdown","metadata":{"id":"jF-ZDDGVdKpj"},"source":["## Dataset"]},{"cell_type":"markdown","metadata":{"id":"v99yxPRydKpk"},"source":["\n","La base de datos del MNIST contiene 60.000 imágenes de entrenamiento y 10.000 imágenes de prueba. La mitad del conjunto de entrenamiento y la otra mitad del conjunto de pruebas se tomaron del conjunto de datos de entrenamiento del NIST, mientras que la otra mitad del conjunto de entrenamiento y la otra mitad del conjunto de pruebas se tomaron del conjunto de datos de pruebas del NIST.Los creadores originales de la base de datos mantienen una lista de algunos de los métodos probados en ella. En su papel original, utilizan una máquina de soporte vectorial para obtener una tasa de error del 0,8%. En 2017 se ha publicado un conjunto de datos ampliado similar al MNIST llamado EMNIST, que contiene 240.000 imágenes de entrenamiento y 40.000 imágenes de prueba de dígitos y caracteres escritos a mano [https://datascience.eu/es/procesamiento-del-lenguaje-natural/base-de-datos-del-mnist/\n","]."]},{"cell_type":"markdown","metadata":{"id":"z4BI0li8dKpk"},"source":["## 1. Lectura del conjunto de datos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nL2uWpdedKpl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686614285852,"user_tz":240,"elapsed":70818,"user":{"displayName":"Diego Antonio Yañez Oyarce","userId":"00726383136312179337"}},"outputId":"c2834386-175f-4cc7-db74-0af6bb03195e"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n","  warn(\n"]}],"source":["# Importamos el conjunto de datos\n","from sklearn.datasets import fetch_openml\n","\n","mnist = fetch_openml('mnist_784')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YNvJPK2sdKpl","colab":{"base_uri":"https://localhost:8080/","height":721},"executionInfo":{"status":"ok","timestamp":1686614288755,"user_tz":240,"elapsed":252,"user":{"displayName":"Diego Antonio Yañez Oyarce","userId":"00726383136312179337"}},"outputId":"1fc1d6ba-6432-4096-ce1f-91a9835137e1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["       pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n","0         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n","1         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n","2         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n","3         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n","4         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n","...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n","69995     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n","69996     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n","69997     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n","69998     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n","69999     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n","\n","       pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n","0          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n","1          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n","2          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n","3          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n","4          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n","...        ...  ...       ...       ...       ...       ...       ...   \n","69995      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n","69996      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n","69997      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n","69998      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n","69999      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n","\n","       pixel780  pixel781  pixel782  pixel783  pixel784  \n","0           0.0       0.0       0.0       0.0       0.0  \n","1           0.0       0.0       0.0       0.0       0.0  \n","2           0.0       0.0       0.0       0.0       0.0  \n","3           0.0       0.0       0.0       0.0       0.0  \n","4           0.0       0.0       0.0       0.0       0.0  \n","...         ...       ...       ...       ...       ...  \n","69995       0.0       0.0       0.0       0.0       0.0  \n","69996       0.0       0.0       0.0       0.0       0.0  \n","69997       0.0       0.0       0.0       0.0       0.0  \n","69998       0.0       0.0       0.0       0.0       0.0  \n","69999       0.0       0.0       0.0       0.0       0.0  \n","\n","[70000 rows x 784 columns]"],"text/html":["\n","  <div id=\"df-5435222b-9d6f-44a7-9494-f3100375d69b\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pixel1</th>\n","      <th>pixel2</th>\n","      <th>pixel3</th>\n","      <th>pixel4</th>\n","      <th>pixel5</th>\n","      <th>pixel6</th>\n","      <th>pixel7</th>\n","      <th>pixel8</th>\n","      <th>pixel9</th>\n","      <th>pixel10</th>\n","      <th>...</th>\n","      <th>pixel775</th>\n","      <th>pixel776</th>\n","      <th>pixel777</th>\n","      <th>pixel778</th>\n","      <th>pixel779</th>\n","      <th>pixel780</th>\n","      <th>pixel781</th>\n","      <th>pixel782</th>\n","      <th>pixel783</th>\n","      <th>pixel784</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>69995</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>69996</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>69997</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>69998</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>69999</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>70000 rows × 784 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5435222b-9d6f-44a7-9494-f3100375d69b')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-5435222b-9d6f-44a7-9494-f3100375d69b button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-5435222b-9d6f-44a7-9494-f3100375d69b');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":2},{"output_type":"stream","name":"stdout","text":["Warning: Total number of columns (784) exceeds max_columns (20) limiting to first (20) columns.\n","Warning: total number of rows (70000) exceeds max_rows (20000). Limiting to first (20000) rows.\n"]}],"source":["# Conviertiendo el conjunto de datos en un DataFrame de Pandas\n","import pandas as pd\n","\n","df = pd.DataFrame(mnist.data)\n","df"]},{"cell_type":"markdown","metadata":{"id":"S--L1TOqdKpm"},"source":["## 2. División del conjunto de datos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X9BDqwmmdKpm"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(mnist.data, mnist.target, test_size=0.15)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J7YyYPcbdKpm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686614542437,"user_tz":240,"elapsed":228,"user":{"displayName":"Diego Antonio Yañez Oyarce","userId":"00726383136312179337"}},"outputId":"77c6c87f-cfae-45fb-980a-a508f52534c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["59500\n","10500\n"]}],"source":["print(len(X_train))\n","print(len(X_test))"]},{"cell_type":"markdown","metadata":{"id":"9RT6TSGHdKpm"},"source":["## 3. Entrenamiento del algoritmo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2AU39NP7dKpm","colab":{"base_uri":"https://localhost:8080/","height":75},"executionInfo":{"status":"ok","timestamp":1686614846545,"user_tz":240,"elapsed":300261,"user":{"displayName":"Diego Antonio Yañez Oyarce","userId":"00726383136312179337"}},"outputId":"b1d39211-5c19-41f3-82da-c8745ea17f45"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["MLPClassifier(activation='logistic', hidden_layer_sizes=(10,), solver='sgd')"],"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, hidden_layer_sizes=(10,), solver=&#x27;sgd&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, hidden_layer_sizes=(10,), solver=&#x27;sgd&#x27;)</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":5}],"source":["from sklearn.neural_network import MLPClassifier\n","\n","clf = MLPClassifier(hidden_layer_sizes=(10,), activation='logistic', solver='sgd')\n","clf.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aKym7FHvdKpn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686614957233,"user_tz":240,"elapsed":261,"user":{"displayName":"Diego Antonio Yañez Oyarce","userId":"00726383136312179337"}},"outputId":"782b65a7-b709-4b4f-8a96-f33e76f28daa"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{},"execution_count":6}],"source":["# Número de capas del perceptrón multicapa\n","clf.n_layers_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OAtzUmu9dKpn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686614958898,"user_tz":240,"elapsed":235,"user":{"displayName":"Diego Antonio Yañez Oyarce","userId":"00726383136312179337"}},"outputId":"d3014523-990b-4843-e66e-c9af94ae1e3d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["10"]},"metadata":{},"execution_count":7}],"source":["# Número de outputs del perceptrón multicapa\n","clf.n_outputs_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wpa09LhddKpn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686615021217,"user_tz":240,"elapsed":245,"user":{"displayName":"Diego Antonio Yañez Oyarce","userId":"00726383136312179337"}},"outputId":"592449c1-ad9d-4780-a026-cdd527afb8d2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["7850"]},"metadata":{},"execution_count":8}],"source":["784 * 10 + 10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lKVAUL1TdKpn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686615048071,"user_tz":240,"elapsed":270,"user":{"displayName":"Diego Antonio Yañez Oyarce","userId":"00726383136312179337"}},"outputId":"f3768220-988c-459a-be0e-a9163a1f9ecc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(784, 10)"]},"metadata":{},"execution_count":9}],"source":["# Número de parámetros que forman el modelo\n","clf.coefs_[0].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0XANuIIMdKpn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686615052749,"user_tz":240,"elapsed":242,"user":{"displayName":"Diego Antonio Yañez Oyarce","userId":"00726383136312179337"}},"outputId":"77f7a333-360d-4399-d18e-7094e8700a92"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 1.09837394, -1.05859867,  1.14942484,  0.76873921,  0.82064565,\n","         1.45240091, -1.90462305, -1.76397917,  1.16470733, -1.30456433],\n","       [ 1.22155648,  1.57701954,  0.68040658, -1.12279803, -1.08858528,\n","         1.46875386,  1.72683098, -1.32815721, -1.30852753, -1.53946316],\n","       [-1.24335688,  1.067107  , -1.09474248, -0.61530257,  2.99882688,\n","        -1.08193366, -1.35947246,  2.27234293, -2.81186237,  2.00432847],\n","       [-1.19386783,  2.47822081,  1.1334568 ,  2.33371605, -1.82378927,\n","        -1.22150171,  1.38370911,  0.13420277, -0.63848354, -1.98877958],\n","       [-1.65544791, -0.72518453, -1.83073158,  1.97406413, -0.36348622,\n","         2.01456707, -0.87379639, -1.13041677,  1.15400824,  0.41255834],\n","       [ 2.91959016, -0.73595634, -0.45005809, -0.19985232, -0.94104821,\n","        -0.58336961, -0.78106048,  2.54134247, -0.85513179, -1.41003797],\n","       [ 1.38323897, -2.68108645,  0.71643005, -0.69390828, -1.01097492,\n","        -0.57311356,  1.56973488,  0.07076639, -0.49279976,  1.26864842],\n","       [-1.04281487,  2.29203144, -0.28077664, -0.20519565, -2.20264561,\n","        -0.2290477 , -2.11061403,  1.57167167,  0.94895642,  2.02430336],\n","       [-0.88424232, -0.94512866,  2.8640903 ,  2.14492496, -0.10129951,\n","        -0.82530897, -2.14243639,  1.60500255, -1.03491506, -0.97386222],\n","       [-1.70262201,  0.54726825,  0.75727384, -2.05512354,  1.80873649,\n","        -1.82513751,  1.48078609, -0.88771389,  2.80059821,  0.35555078]])"]},"metadata":{},"execution_count":10}],"source":["# Dimensiones de la primera capa (hidden layer)\n","clf.coefs_[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"piPYzhQ4dKpn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686615059591,"user_tz":240,"elapsed":261,"user":{"displayName":"Diego Antonio Yañez Oyarce","userId":"00726383136312179337"}},"outputId":"8646af06-dd26-4cb7-9f4e-12619a024df1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10, 10)"]},"metadata":{},"execution_count":11}],"source":["# Dimensiones de la segunda capa (output layer)\n","clf.coefs_[1].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CAPu8muzdKpn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686615066337,"user_tz":240,"elapsed":277,"user":{"displayName":"Diego Antonio Yañez Oyarce","userId":"00726383136312179337"}},"outputId":"cdade8ed-b92a-4ec0-f071-f0332c052909"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10,)"]},"metadata":{},"execution_count":12}],"source":["# Parametros bias/intercept que forman parte de cada capa de la red neuronal\n","clf.intercepts_[0].shape"]},{"cell_type":"markdown","metadata":{"id":"2vnnetpYdKpo"},"source":["## 4. Predicción con el conjunto de pruebas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fgiGUmjedKpo"},"outputs":[],"source":["# Realizamos la predicción con el conjunto de datos de prueba\n","y_pred = clf.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q5iVEPpxdKpo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686615104119,"user_tz":240,"elapsed":280,"user":{"displayName":"Diego Antonio Yañez Oyarce","userId":"00726383136312179337"}},"outputId":"5bc36de6-6199-445c-dd0d-4542d7cd2191"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8914130709035303"]},"metadata":{},"execution_count":14}],"source":["# Mostramos el f1_score resultante de la clasificación\n","from sklearn.metrics import f1_score\n","\n","f1_score(y_test, y_pred, average=\"weighted\")"]},{"cell_type":"markdown","metadata":{"id":"CzYh4FA6dKpo"},"source":["### Volver a repetir el ejercicio pero aumentando el numero de neuronas en la hidden layer"]},{"cell_type":"markdown","metadata":{"id":"If_gfgFOdKpo"},"source":["## 6. Mostrando las imagenes mal clasificadas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VDBNCUh4dKpo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686615253673,"user_tz":240,"elapsed":251,"user":{"displayName":"Diego Antonio Yañez Oyarce","userId":"00726383136312179337"}},"outputId":"ea3c2e22-71f8-4295-f59a-0cd4da5a1014"},"outputs":[{"output_type":"stream","name":"stdout","text":["[7, 23, 36, 74, 77, 95, 99, 103, 115, 130, 140, 142, 150, 171, 172, 182, 204, 225, 226, 238, 268, 277, 283, 284, 300, 302, 336, 344, 359, 360, 379, 393, 402, 418, 425, 427, 436, 448, 469, 477, 482, 485, 491, 496, 499, 527, 544, 547, 571, 586, 601, 620, 621, 628, 644, 653, 656, 659, 682, 683, 693, 698, 700, 704, 729, 741, 752, 770, 774, 780, 783, 796, 805, 808, 809, 818, 819, 825, 833, 861, 872, 886, 893, 906, 916, 918, 921, 924, 939, 948, 952, 962, 964, 985, 995, 1006, 1009, 1025, 1026, 1038, 1042, 1072, 1088, 1090, 1094, 1095, 1099, 1105, 1106, 1116, 1122, 1134, 1140, 1141, 1142, 1172, 1195, 1196, 1201, 1204, 1225, 1229, 1232, 1236, 1252, 1263, 1267, 1285, 1292, 1295, 1296, 1310, 1325, 1330, 1339, 1349, 1389, 1390, 1407, 1414, 1421, 1431, 1433, 1434, 1450, 1483, 1487, 1497, 1498, 1500, 1501, 1504, 1508, 1530, 1537, 1549, 1565, 1578, 1580, 1581, 1594, 1609, 1614, 1615, 1619, 1657, 1661, 1668, 1677, 1682, 1684, 1694, 1703, 1709, 1712, 1719, 1743, 1751, 1752, 1762, 1763, 1772, 1789, 1791, 1819, 1822, 1824, 1846, 1854, 1857, 1860, 1893, 1910, 1919, 1920, 1925, 1927, 1941, 1947, 1956, 1960, 1963, 1969, 1970, 1981, 1984, 1987, 1995, 2011, 2032, 2060, 2067, 2070, 2073, 2077, 2084, 2094, 2100, 2102, 2104, 2119, 2141, 2142, 2168, 2170, 2178, 2184, 2192, 2193, 2201, 2233, 2243, 2268, 2277, 2312, 2322, 2343, 2347, 2351, 2358, 2379, 2389, 2390, 2406, 2432, 2453, 2454, 2455, 2464, 2468, 2472, 2482, 2485, 2492, 2497, 2502, 2506, 2526, 2529, 2538, 2546, 2582, 2586, 2601, 2619, 2624, 2654, 2659, 2663, 2676, 2689, 2697, 2710, 2744, 2755, 2786, 2787, 2788, 2797, 2798, 2801, 2808, 2812, 2833, 2836, 2853, 2861, 2884, 2890, 2899, 2903, 2906, 2918, 2932, 2935, 2938, 2946, 2958, 2970, 2975, 2988, 2997, 3027, 3034, 3040, 3046, 3054, 3066, 3075, 3076, 3080, 3083, 3085, 3099, 3100, 3118, 3128, 3139, 3141, 3176, 3179, 3205, 3232, 3274, 3281, 3285, 3290, 3298, 3309, 3311, 3315, 3321, 3327, 3328, 3338, 3344, 3346, 3359, 3363, 3366, 3368, 3372, 3374, 3388, 3399, 3403, 3427, 3460, 3484, 3485, 3497, 3521, 3523, 3526, 3550, 3554, 3564, 3580, 3590, 3605, 3615, 3632, 3651, 3660, 3664, 3669, 3671, 3672, 3687, 3688, 3689, 3709, 3716, 3758, 3771, 3773, 3775, 3776, 3781, 3791, 3811, 3818, 3828, 3830, 3838, 3850, 3883, 3930, 3949, 3965, 3968, 3973, 3974, 3980, 3989, 3992, 4000, 4004, 4007, 4011, 4044, 4053, 4062, 4080, 4098, 4102, 4103, 4136, 4139, 4140, 4143, 4144, 4157, 4165, 4176, 4205, 4218, 4222, 4225, 4227, 4241, 4246, 4258, 4261, 4270, 4271, 4283, 4287, 4288, 4294, 4301, 4303, 4321, 4335, 4336, 4355, 4356, 4360, 4365, 4369, 4371, 4378, 4380, 4389, 4402, 4413, 4417, 4421, 4431, 4436, 4438, 4439, 4451, 4454, 4455, 4484, 4485, 4494, 4496, 4505, 4522, 4525, 4528, 4536, 4554, 4565, 4583, 4585, 4593, 4601, 4604, 4608, 4611, 4612, 4616, 4626, 4632, 4644, 4654, 4658, 4659, 4665, 4671, 4690, 4691, 4696, 4702, 4712, 4714, 4716, 4717, 4724, 4740, 4744, 4750, 4752, 4767, 4774, 4780, 4783, 4785, 4791, 4795, 4797, 4819, 4839, 4845, 4859, 4864, 4866, 4870, 4871, 4874, 4875, 4884, 4896, 4902, 4903, 4904, 4913, 4934, 4940, 4942, 4955, 4974, 4985, 4986, 4989, 4992, 4997, 5020, 5028, 5043, 5051, 5079, 5084, 5088, 5106, 5108, 5111, 5127, 5147, 5168, 5178, 5189, 5192, 5198, 5212, 5214, 5215, 5220, 5223, 5234, 5238, 5240, 5241, 5258, 5269, 5285, 5296, 5328, 5335, 5348, 5355, 5367, 5372, 5377, 5378, 5394, 5405, 5408, 5417, 5421, 5429, 5434, 5436, 5442, 5445, 5451, 5461, 5469, 5470, 5477, 5499, 5515, 5528, 5540, 5541, 5543, 5544, 5560, 5564, 5574, 5575, 5609, 5611, 5615, 5647, 5649, 5652, 5659, 5660, 5666, 5687, 5698, 5712, 5732, 5745, 5757, 5766, 5774, 5776, 5782, 5792, 5807, 5816, 5836, 5853, 5860, 5876, 5879, 5892, 5897, 5926, 5928, 5933, 5950, 5960, 5973, 5974, 5979, 6009, 6048, 6056, 6057, 6059, 6069, 6071, 6074, 6097, 6113, 6122, 6130, 6136, 6149, 6154, 6158, 6167, 6171, 6172, 6173, 6175, 6198, 6200, 6204, 6207, 6221, 6223, 6224, 6227, 6232, 6234, 6244, 6251, 6293, 6299, 6301, 6307, 6319, 6321, 6351, 6361, 6386, 6395, 6415, 6417, 6423, 6430, 6468, 6475, 6477, 6480, 6488, 6489, 6491, 6520, 6529, 6537, 6543, 6547, 6552, 6564, 6602, 6626, 6653, 6657, 6679, 6685, 6693, 6706, 6717, 6718, 6723, 6730, 6736, 6739, 6746, 6756, 6758, 6760, 6770, 6772, 6785, 6803, 6806, 6811, 6817, 6825, 6826, 6837, 6843, 6859, 6860, 6870, 6871, 6873, 6891, 6902, 6904, 6921, 6932, 6943, 6969, 6975, 6981, 6990, 6991, 7023, 7043, 7044, 7068, 7077, 7080, 7090, 7094, 7117, 7126, 7137, 7138, 7141, 7142, 7143, 7149, 7154, 7156, 7168, 7169, 7207, 7223, 7229, 7230, 7234, 7252, 7255, 7267, 7270, 7272, 7278, 7281, 7286, 7296, 7304, 7314, 7317, 7318, 7323, 7327, 7329, 7330, 7340, 7356, 7360, 7362, 7363, 7366, 7371, 7380, 7384, 7387, 7389, 7404, 7405, 7438, 7439, 7453, 7463, 7477, 7478, 7481, 7489, 7491, 7513, 7516, 7523, 7542, 7546, 7552, 7554, 7558, 7564, 7571, 7573, 7577, 7581, 7608, 7616, 7623, 7624, 7625, 7631, 7632, 7644, 7648, 7678, 7688, 7700, 7714, 7716, 7718, 7746, 7752, 7759, 7784, 7800, 7803, 7816, 7822, 7856, 7871, 7872, 7874, 7883, 7890, 7893, 7909, 7918, 7919, 7922, 7949, 7956, 7960, 7974, 7981, 7989, 7997, 7998, 8002, 8009, 8012, 8019, 8029, 8034, 8045, 8048, 8049, 8050, 8068, 8071, 8104, 8107, 8109, 8112, 8120, 8126, 8145, 8149, 8163, 8164, 8182, 8183, 8184, 8193, 8196, 8220, 8230, 8236, 8247, 8249, 8258, 8261, 8267, 8273, 8274, 8276, 8287, 8291, 8301, 8304, 8311, 8318, 8335, 8342, 8348, 8360, 8366, 8377, 8385, 8394, 8397, 8407, 8411, 8416, 8419, 8479, 8481, 8484, 8496, 8500, 8517, 8521, 8525, 8529, 8543, 8550, 8563, 8566, 8577, 8581, 8589, 8609, 8610, 8616, 8622, 8645, 8675, 8688, 8691, 8693, 8695, 8697, 8702, 8721, 8726, 8733, 8752, 8759, 8769, 8774, 8782, 8793, 8801, 8807, 8836, 8839, 8847, 8849, 8858, 8862, 8885, 8901, 8902, 8911, 8912, 8913, 8918, 8927, 8933, 8936, 8958, 8959, 8961, 8962, 8963, 8988, 9032, 9038, 9056, 9058, 9062, 9080, 9088, 9100, 9102, 9103, 9108, 9114, 9121, 9125, 9126, 9127, 9132, 9138, 9142, 9148, 9152, 9184, 9187, 9191, 9193, 9211, 9221, 9232, 9235, 9236, 9246, 9248, 9249, 9251, 9254, 9265, 9271, 9280, 9286, 9290, 9346, 9352, 9353, 9365, 9373, 9374, 9390, 9391, 9392, 9394, 9403, 9413, 9416, 9421, 9432, 9438, 9442, 9452, 9463, 9473, 9506, 9522, 9523, 9525, 9533, 9551, 9553, 9571, 9582, 9584, 9586, 9587, 9606, 9617, 9618, 9644, 9656, 9666, 9668, 9676, 9681, 9685, 9721, 9723, 9739, 9743, 9746, 9758, 9761, 9775, 9789, 9795, 9804, 9813, 9819, 9826, 9845, 9848, 9865, 9874, 9884, 9892, 9910, 9921, 9925, 9926, 9942, 9943, 9984, 10003, 10004, 10011, 10014, 10054, 10056, 10058, 10060, 10066, 10075, 10087, 10098, 10112, 10121, 10128, 10142, 10153, 10161, 10177, 10180, 10182, 10202, 10203, 10211, 10235, 10238, 10244, 10247, 10260, 10264, 10270, 10298, 10300, 10301, 10313, 10319, 10332, 10345, 10369, 10373, 10382, 10401, 10407, 10417, 10419, 10435, 10440, 10442, 10445, 10446, 10448, 10465, 10468, 10485]\n","1137\n","10500\n"]}],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","index = 0\n","index_errors = []\n","total = 0\n","for label, predict in zip(y_test, y_pred):\n","    if label != predict:\n","        index_errors.append(index)\n","    index += 1\n","    total +=1\n","\n","print(index_errors)\n","print(len(index_errors))\n","print(total)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}